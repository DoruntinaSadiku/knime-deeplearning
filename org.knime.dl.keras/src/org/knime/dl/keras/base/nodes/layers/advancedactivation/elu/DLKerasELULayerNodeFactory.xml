<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE knimeNode PUBLIC "-//UNIKN//DTD KNIME Node 2.0//EN" "http://www.knime.org/Node.dtd">
<knimeNode icon="dlkeraselulayer.png" type="Other">
	<name>Keras ELU Layer</name>

	<shortDescription>
		An exponential linear unit.
	</shortDescription>

	<fullDescription>
		<intro>
			Exponential linear units were introduced to alleviate the disadvantages of
			ReLU and LeakyReLU units, namely to push the mean activation closer to zero while
			still saturating to a negative value which increases robustness against noise if the unit is
			in an off state (i.e. the input is very negative).
			The formula is <tt>f(x) = alpha * (exp(x) - 1) for x &lt; 0 and f(x) = x for x >= 0</tt>.
			For the exact details see the corresponding <a href="https://arxiv.org/abs/1511.07289v1">paper</a>.
		</intro>
		<option name="Alpha">
			Scale for the negative factor of the exponential linear unit.
		</option>
	</fullDescription>
	<ports>
		<inPort index="0" name="Deep Learning Network">
			The Keras deep learning network to which to add a
			<tt>ELU</tt>
			layer.
		</inPort>
		<outPort index="0" name="Deep Learning Network">
			The Keras deep learning network with an added
			<tt>ELU</tt>
			layer.
		</outPort>
	</ports>
</knimeNode>
