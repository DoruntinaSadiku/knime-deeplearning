<?xml version="1.0" encoding="UTF-8"?>
<knimeNode icon="dlkeraslearner.png" type="Learner" xmlns="http://knime.org/node/v2.8" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://knime.org/node/v2.10 http://knime.org/node/v2.10.xsd">
	<name>DL Keras Network Learner</name>

	<shortDescription>
		Performs supervised training on a Keras deep learning
		network.
	</shortDescription>

	<fullDescription>
		<intro>
			This node performs supervised training on a Keras deep learning
			network.
		</intro>
		<tab name="General Settings">
			<option name="Back end">
				The deep learning back end which is used to
				train
				the input network.
			</option>
			<option name="Epochs">
				The number of iterations over the input training
				data.
			</option>
			<option name="Batch size">
				The number of rows that are used for a single
				gradient update.
			</option>
		</tab>
		<tab name="Optimizer Settings">
			<option name="Optimizer">
				The optimization algorithm. The following optimizers are available:
				<ul>
					<li>
						<a href="https://keras.io/optimizers/#adadelta">Adadelta</a>
					</li>
					<li>
						<a href="https://keras.io/optimizers/#adagrad">Adagrad</a>
					</li>
					<li>
						<a href="https://keras.io/optimizers/#adam">Adam</a>
					</li>
					<li>
						<a href="https://keras.io/optimizers/#adamax">Adamax</a>
					</li>
					<li>
						<a href="https://keras.io/optimizers/#nadam">Nadam</a>
					</li>
					<li>
						<a
							href="https://keras.io/optimizers/#rmsprop">RMSProp</a>
					</li>
					<li>
						<a href="https://keras.io/optimizers/#rsgd">Stochastic gradient descent</a>
					</li>
				</ul>
				Please refer to <a href="https://keras.io/optimizers/">the Keras documentation</a> for further information on
				parameterization.
			</option>
			<option name="Clip norm">
				If checked, gradients whose L2 norm exceeds the
				given norm will be clipped to that norm.
			</option>
			<option name="Clip value">
				If checked, gradients whose absolute value
				exceeds the given value will be clipped to that value (or the
				negated value, respectively).
			</option>
		</tab>
		<tab name="Learning Behavior">
			<option name="Terminate on NaN loss">
				If checked, training is terminated when a NaN loss is encountered,
				i.e. when the loss is numerically unstable.
				Corresponds to the
				<a href="https://keras.io/callbacks/#terminateonnan">TerminateOnNaN Keras callback</a>.
			</option>
			<option name="Terminate on training stagnation (early stopping)">
				If checked, training is terminated when the loss has stopped improving.
				<ul>
					<li>Min. delta: minimum change of the loss which qualifies as an improvement.
					Absolute changes below this value are considered a stagnation.</li>
					<li>Patience: number of epochs with no improvements after which training will be stopped.</li>
				</ul>
				Corresponds to the
				<a href="https://keras.io/callbacks/#earlystopping">EarlyStopping Keras callback</a>.
			</option>
			<option name="Reduce learning rate on training stagnation">
				If checked, the learning rate is reduced when the loss has stopped improving.
				<ul>
					<li>Factor: factor by which the learning rate will be reduced</li>
					<li>Patience: number of epochs with no improvements after which the learning rate will be reduced.</li>
					<li>Epsilon: threshold for measuring the new optimum, to only focus on significant changes.</li>
					<li>Cooldown: number of epochs to wait before resuming normal operation after the learning rate has been reduced.</li>
					<li>Min. learning rate: lower bound of the learning rate. The learning rate is not reduced below this value.</li>
				</ul>
				Corresponds to the
				<a href="https://keras.io/callbacks/#reducelronplateau">ReduceLROnPlateau Keras callback</a>.
			</option>
		</tab>
		<tab name="Training Inputs">
			<option name="Training input">
				For each input, the following options
				must be set:
			</option>
			<option name="Conversion">
				The converter that is used to transform the
				selected input columns into a format that is accepted by the
				respective network input specification.
			</option>
			<option name="Input columns">
				The table columns that are part of the respective
				network input.
				The availability of a column depends on the currently
				selected input converter.
			</option>
		</tab>
		<tab name="Training Targets">
			<option name="Training target">
				For each target, the following options
				must be set:
			</option>
			<option name="Conversion">
				The converter that is used to transform the
				selected target columns into a format that is accepted by the
				respective network target specification.
			</option>
			<option name="Target columns">
				The table columns that are part of the respective
				network target.
				The availability of a column depends on the currently
				selected input converter.
			</option>
			<option name="Loss function">
				The loss function for the network target.
			</option>
		</tab>
	</fullDescription>

	<ports>
		<inPort index="0" name="Deep Learning Network">The input Keras deep learning network.
		</inPort>
		<inPort index="1" name="Data Table">The input table that contains training
			and target columns.</inPort>
		<outPort index="0" name="Deep Learning Network">The trained output Keras deep learning
			network.
		</outPort>
	</ports>

	<views>
		<view index="0" name="Learning Monitor">
			Shows information about the current learning run. Has an option for early
			stopping of training. If training
			is stopped before the last epoch the model will be saved in the current
			status.
		</view>
	</views>  
</knimeNode>
